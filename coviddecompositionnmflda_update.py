# -*- coding: utf-8 -*-
"""CovidDecompositionNMFLDA_Update.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SngUwEC_P3iVT50Dv6ZFeKyF44A8ZCjZ

# **CovidModeling**
"""

# importing the libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate

import warnings
warnings.filterwarnings('ignore')

# load the data
df = pd.read_excel("dataset1.xlsx")

# lets try to look the data
df.head()

df.shape

df['length_text'] = df['text'].str.len()
sns.distplot(df['length_text'], color="r")
plt.show()

# importing the NLP libraries that will be used for preprocessing
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
# stopwords.
from nltk.probability import FreqDist
from wordcloud import WordCloud

stopwords.words("english")[:10] # <-- import the english stopwords

def preprocess_text(text: str, remove_stopwords: bool) -> str:
    """This utility function sanitizes a string by:
    - removing links
    - removing special characters
    - removing numbers
    - removing stopwords
    - transforming in lowercase
    - removing excessive whitespaces
    Args:
        text (str): the input text you want to clean
        remove_stopwords (bool): whether or not to remove stopwords
    Returns:
        str: the cleaned text
    """

    # remove links
    text = re.sub(r"http\S+", "", text)
    # remove special chars and numbers
    text = re.sub("[^A-Za-z]+", " ", text)
    # remove stopwords
    if remove_stopwords:
        # 1. tokenize
        tokens = nltk.word_tokenize(text)
        # 2. check if stopword
        tokens = [w for w in tokens if not w.lower() in stopwords.words("english") if len(w)>=3]
        # 3. join back together
        text = " ".join(tokens)
    # return text in lower case and stripped of whitespaces
    text = text.lower().strip()
    return text

# define a function for getting all words from the text
def returning_tokinize_list(df,column_name):
    df = df.reset_index(drop=True)  
    tokenize_list = [word_tokenize(df[column_name][i]) for i in range(df.shape[0])]
    final = [j for i in tokenize_list for j in i]
    return final

df['cleaned'] = df['text'].apply(lambda x: preprocess_text(x, remove_stopwords=True))

# calling a function
tokenize_list_words = returning_tokinize_list(df, 'cleaned')

# function for extracting the most common words in reviews text
def most_common_words(cleaned_col_name_list,common_words = 10):
    fdist = FreqDist(cleaned_col_name_list)
    most_common=fdist.most_common(common_words)
    return most_common


# plotting the graph of most common words
def frequency_dis_graph(cleaned_col_name_list,num_of_words=10):
    fdist = FreqDist(cleaned_col_name_list)
    fdist.plot(num_of_words,cumulative=False, marker='o')
    plt.show()


# draw a graph of word which are most common
def word_cloud(data):
    unique_string=(" ").join(data)
    wordcloud = WordCloud(width = 1000, height = 500).generate(unique_string)
    plt.figure(figsize=(15,8))
    plt.imshow(wordcloud)
    plt.axis("off")
    plt.savefig("your_file_name"+".png", bbox_inches='tight')
    plt.show()
    plt.close()

# function for words in dataframe format
def table_format(data_list,column_name):
    df_ = pd.DataFrame(data_list, columns = [column_name,'Frequency_distribution'])
    return df_

# draw word cloud
word_cloud(tokenize_list_words)

# lets try to check the 10 most common words
MCW = most_common_words(tokenize_list_words)
table_format(MCW, 'text')

# graph for showing top 10 most common words
frequency_dis_graph(tokenize_list_words)

df = df[['text', 'cleaned']]

df

# Import classifiers
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB

import time

from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, classification_report,confusion_matrix

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from sklearn.decomposition import NMF, LatentDirichletAllocation

"""#**TF-IDF**"""

# use tfidf by removing tokens that don't appear in at least 50 documents
vect = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)
 
# Fit and transform
X = vect.fit_transform(df.cleaned)

"""# **Non-negative Matrix Factorization**"""

# Create an NMF instance: model
# the 10 components will be the topics
model = NMF(n_components=10, random_state=5)
 
# Fit the model to TF-IDF
model.fit(X)
 
# Transform the TF-IDF: nmf_features
nmf_features = model.transform(X)

# TF-IDF dimension
X.shape

# Features Dimensions:
nmf_features.shape

# Components Dimensions:
model.components_.shape

# Create a DataFrame: components_df
components_df = pd.DataFrame(model.components_, columns=vect.get_feature_names())
components_df

for topic in range(components_df.shape[0]):
    tmp = components_df.iloc[topic]
    print(f'For topic {topic+1} the words with the highest value are:')
    print(tmp.nlargest(10))
    print('\n')

for topic in range(components_df.shape[0]):
    print(f'For topic {topic+1}:')
    components_df.iloc[topic].nlargest(50).sort_values().plot.barh(figsize=(15,15))
    plt.show()

df_ = components_df
df_.index = ['topic'+str(i+1) for i in range(10)]

import plotly.express as px

plt.figure(figsize=(15,15))
# df = px.data.gapminder().query("continent == 'Europe' and year == 2007 and pop > 2.e6")
fig = px.bar(df_, x=df_.columns.tolist(),
            title="NMF Topic Modeling")
fig.show()

"""# **Latent Dirichlet Allocation**"""

# Create an NMF instance: model
# the 10 components will be the topics
model = LatentDirichletAllocation(n_components=10, random_state=5)
 
# Fit the model to TF-IDF
model.fit(X)
 
# Transform the TF-IDF: lda_features
lda_features = model.transform(X)

# Create a DataFrame: components_df
components_df = pd.DataFrame(model.components_, columns=vect.get_feature_names())
components_df

for topic in range(components_df.shape[0]):
    tmp = components_df.iloc[topic]
    print(f'For topic {topic+1} the words with the highest value are:')
    print(tmp.nlargest(10))
    print('\n')

for topic in range(components_df.shape[0]):
    print(f'For topic {topic+1}:')
    components_df.iloc[topic].nlargest(50).sort_values().plot.barh(figsize=(15,15))
    plt.show()

df_ = components_df
df_.index = ['topic'+str(i+1) for i in range(10)]

import plotly.express as px

plt.figure(figsize=(15,15))
# df = px.data.gapminder().query("continent == 'Europe' and year == 2007 and pop > 2.e6")
fig = px.bar(df_, x=df_.columns.tolist(),
            title="LDA Topic Modeling")
fig.show()